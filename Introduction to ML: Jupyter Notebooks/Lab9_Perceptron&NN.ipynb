{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMXXKU1HF2v+8kg9NfBup6K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Task 1 - AND Gate Perceptron Model"],"metadata":{"id":"ILo69ncfuYrt"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BfvDNfXqq1bt"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","source":["def activation_function(x):\n","    return 1 if x >= 0 else 0\n","\n","def perceptron_model(x, w, b):\n","    return activation_function(np.dot(x, w) + b)"],"metadata":{"id":"9yRekaY7uqLW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","Y = np.array([0, 0, 0, 1])"],"metadata":{"id":"zoBQZk07utXc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["learning_rate = 0.1\n","epochs = 10\n","\n","w = np.array([0.5, 0.5])\n","b = 0.0"],"metadata":{"id":"iVQ6-a5buvZL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Perceptron Model:"],"metadata":{"id":"9veRxmvpu0WE"}},{"cell_type":"code","source":["for epoch in range(epochs):\n","    print(\"Epoch:\", epoch)\n","    print(\"  w:\", w, \"  b:\", b)\n","    for i in range(len(X)):\n","        x = X[i]\n","        y = Y[i]\n","        y_pred = perceptron_model(x, w, b)\n","        error = y - y_pred\n","        w = w + learning_rate * error * x\n","        b = b + learning_rate * error"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uhVDcczGrRbO","executionInfo":{"status":"ok","timestamp":1689006922083,"user_tz":-330,"elapsed":15,"user":{"displayName":"Ashima Fatima 21BAI1830","userId":"02955847702660334011"}},"outputId":"941187b6-0357-451c-d20a-76262be74ff7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0\n","  w: [0.5 0.5]   b: 0.0\n","Epoch: 1\n","  w: [0.4 0.4]   b: -0.30000000000000004\n","Epoch: 2\n","  w: [0.3 0.3]   b: -0.5\n","Epoch: 3\n","  w: [0.3 0.3]   b: -0.5\n","Epoch: 4\n","  w: [0.3 0.3]   b: -0.5\n","Epoch: 5\n","  w: [0.3 0.3]   b: -0.5\n","Epoch: 6\n","  w: [0.3 0.3]   b: -0.5\n","Epoch: 7\n","  w: [0.3 0.3]   b: -0.5\n","Epoch: 8\n","  w: [0.3 0.3]   b: -0.5\n","Epoch: 9\n","  w: [0.3 0.3]   b: -0.5\n"]}]},{"cell_type":"markdown","source":["# Task 2 - Iris Dataset"],"metadata":{"id":"65GtxFyFvVRf"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder"],"metadata":{"id":"1Jv8KPrVx8Lf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","encoder = OneHotEncoder()\n","y_encoder = encoder.fit(y_train.reshape(-1, 1))\n","y_train_encoded = y_encoder.transform(y_train.reshape(-1, 1)).toarray()"],"metadata":{"id":"yf2JxIcax--E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sigmoid(x):\n","    # Apply clipping to prevent overflow or underflow\n","    x = np.clip(x, -500, 500)\n","    return 1 / (1 + np.exp(-x))\n","\n","def sigmoid_derivative(x):\n","    return sigmoid(x) * (1 - sigmoid(x))\n","\n","def perceptron_model(x, w, b):\n","    return sigmoid(np.dot(x, w) + b)"],"metadata":{"id":"LL34MASG0Vxj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["training function"],"metadata":{"id":"xvlx2nH9va86"}},{"cell_type":"code","source":["def train(X, y, learning_rate, epochs, hidden_nodes):\n","    input_nodes = X.shape[1]\n","    output_nodes = y.shape[1]\n","    w1 = np.random.randn(input_nodes, hidden_nodes)\n","    # hidden_nodes is number of nodes in hidden layer\n","    b1 = np.zeros((1, hidden_nodes)) #bias vector for the hidden layer\n","    w2 = np.random.randn(hidden_nodes, output_nodes)\n","    b2 = np.zeros((1, output_nodes))\n","\n","    for epoch in range(epochs):\n","        # Forward propagation\n","        z1 = np.dot(X, w1) + b1\n","        a1 = sigmoid(z1)\n","        z2 = np.dot(a1, w2) + b2\n","        a2 = sigmoid(z2)\n","\n","        # Backward propagation\n","        error = y - a2\n","        delta2 = error * sigmoid_derivative(a2)\n","        delta1 = np.dot(delta2, w2.T) * sigmoid_derivative(a1)\n","\n","        w2 += learning_rate * np.dot(a1.T, delta2)\n","        b2 += learning_rate * np.sum(delta2, axis=0, keepdims=True)\n","        w1 += learning_rate * np.dot(X.T, delta1)\n","        b1 += learning_rate * np.sum(delta1, axis=0)\n","\n","    y_pred = np.argmax(a2, axis=1)\n","    accuracy = np.mean(y_pred == y_train)\n","    print(\"Accuracy:\", accuracy)\n","\n","    return w1, b1, w2, b2"],"metadata":{"id":"7Fb1CUZeuF3G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The purpose of using np.argmax() in this code is to convert the output of the neural network, which is a matrix of probabilities, into a vector of predicted class labels. The np.argmax() function selects the index of the maximum probability along each row of the output matrix, which corresponds to the predicted class label for that input. The resulting vector of predicted class labels is then compared to the target variable y_train to calculate the accuracy of the neural network."],"metadata":{"id":"4nHk03ST5eqS"}},{"cell_type":"code","source":["learning_rate = 0.1\n","epochs = 1000\n","hidden_nodes = 10\n","w1, b1, w2, b2 = train(X_train, y_train_encoded, learning_rate, epochs, hidden_nodes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GwyzmRpfuLKH","executionInfo":{"status":"ok","timestamp":1689008224698,"user_tz":-330,"elapsed":378,"user":{"displayName":"Ashima Fatima 21BAI1830","userId":"02955847702660334011"}},"outputId":"d79c4a68-839d-4dd1-ea38-76cdd51c311b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.7916666666666666\n"]}]},{"cell_type":"code","source":["learning_rate = 0.01\n","epochs = 1000\n","hidden_nodes = 10\n","w1, b1, w2, b2 = train(X_train, y_train_encoded, learning_rate, epochs, hidden_nodes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fCGpM5X9vEdr","executionInfo":{"status":"ok","timestamp":1689008247320,"user_tz":-330,"elapsed":375,"user":{"displayName":"Ashima Fatima 21BAI1830","userId":"02955847702660334011"}},"outputId":"fcfff0cf-453e-4593-a322-d6ed1402d9b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.975\n"]}]},{"cell_type":"code","source":["learning_rate = 0.1\n","epochs = 500\n","hidden_nodes = 10\n","w1, b1, w2, b2 = train(X_train, y_train_encoded, learning_rate, epochs, hidden_nodes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ynjUhq43vJsP","executionInfo":{"status":"ok","timestamp":1689008256961,"user_tz":-330,"elapsed":362,"user":{"displayName":"Ashima Fatima 21BAI1830","userId":"02955847702660334011"}},"outputId":"3f16d70b-194f-4f5d-de39-40f5635f2a50"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.6583333333333333\n"]}]},{"cell_type":"code","source":["learning_rate = 0.1\n","epochs = 1000\n","hidden_nodes = 5\n","w1, b1, w2, b2 = train(X_train, y_train_encoded, learning_rate, epochs, hidden_nodes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"85X2jcrfvL33","executionInfo":{"status":"ok","timestamp":1689008266035,"user_tz":-330,"elapsed":1007,"user":{"displayName":"Ashima Fatima 21BAI1830","userId":"02955847702660334011"}},"outputId":"e2ba4e3f-7bc3-42ca-d8cc-2c123751d0b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.3416666666666667\n"]}]},{"cell_type":"markdown","source":["# For 2 hidden layers"],"metadata":{"id":"iu7y7I7a4p5i"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Perform train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize the OneHotEncoder\n","encoder = OneHotEncoder()\n","y_encoder = encoder.fit(y_train.reshape(-1, 1))\n","\n","# Transform the training labels\n","y_train_encoded = y_encoder.transform(y_train.reshape(-1, 1)).toarray()\n","\n","# Define the activation function\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","# Define the derivative of the activation function\n","def sigmoid_derivative(x):\n","    return sigmoid(x) * (1 - sigmoid(x))\n","\n","# Define the perceptron function\n","def perceptron_model(x, w, b):\n","    return sigmoid(np.dot(x, w) + b)\n","\n","# Define the training function\n","def train(X, y, learning_rate, epochs, hidden_nodes1, hidden_nodes2):\n","    input_nodes = X.shape[1]\n","    output_nodes = y.shape[1]\n","    w1 = np.random.randn(input_nodes, hidden_nodes1)\n","    b1 = np.zeros((1, hidden_nodes1))\n","    w2 = np.random.randn(hidden_nodes1, hidden_nodes2)\n","    b2 = np.zeros((1, hidden_nodes2))\n","    w3 = np.random.randn(hidden_nodes2, output_nodes)\n","    b3 = np.zeros((1, output_nodes))\n","\n","    for epoch in range(epochs):\n","        # Forward propagation\n","        z1 = np.dot(X, w1) + b1\n","        a1 = sigmoid(z1)\n","        z2 = np.dot(a1, w2) + b2\n","        a2 = sigmoid(z2)\n","        z3 = np.dot(a2, w3) + b3\n","        a3 = sigmoid(z3)\n","\n","        # Backward propagation\n","        error = y - a3\n","        delta3 = error * sigmoid_derivative(a3)\n","        delta2 = np.dot(delta3, w3.T) * sigmoid_derivative(a2)\n","        delta1 = np.dot(delta2, w2.T) * sigmoid_derivative(a1)\n","\n","        # Weight updates\n","        w3 += learning_rate * np.dot(a2.T, delta3)\n","        b3 += learning_rate * np.sum(delta3, axis=0, keepdims=True)\n","        w2 += learning_rate * np.dot(a1.T, delta2)\n","        b2 += learning_rate * np.sum(delta2, axis=0, keepdims=True)\n","        w1 += learning_rate * np.dot(X.T, delta1)\n","        b1 += learning_rate * np.sum(delta1, axis=0, keepdims=True)\n","\n","    y_pred = np.argmax(a3, axis=1)\n","    accuracy = np.mean(y_pred == y_train)\n","    print(\"Accuracy:\", accuracy)\n","\n","    return w1, b1, w2, b2, w3, b3\n","\n","# Set hyperparameters and train the model\n","learning_rate = 0.1\n","epochs = 1000\n","hidden_nodes1 = 10\n","hidden_nodes2 = 10\n","w1, b1, w2, b2, w3, b3 = train(X_train, y_train_encoded, learning_rate, epochs, hidden_nodes1, hidden_nodes2)\n","\n","# Test the model\n","z1 = np.dot(X_test, w1) + b1\n","a1 = sigmoid(z1)\n","z2 = np.dot(a1, w2) + b2\n","a2 = sigmoid(z2)\n","z3 = np.dot(a2, w3) + b3\n","a3 = sigmoid(z3)\n","y_pred = np.argmax(a3, axis=1)\n","accuracy = np.mean(y_pred == y_test)\n","print(\"Accuracy:\", accuracy)\n"],"metadata":{"id":"zSvwY_Ma4sED"},"execution_count":null,"outputs":[]}]}